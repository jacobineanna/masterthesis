{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20bdae0",
   "metadata": {},
   "source": [
    "# Overview Thesis Code\n",
    "\n",
    "This notebook provides a comprehensive overview of the code used in my master's thesis.\n",
    "\n",
    "Access to the thesis code is available via the GitHub repository linked here: [\\url{https/github.com/jacobineanna/masterthesis}]. Initially, the code was stored in separate Python files. For clarity and ease of understanding, it has been consolidated into a single Jupyter notebook.\n",
    "\n",
    "The core structure of the code draws inspiration from formal Data Science curriculum courses. Additionally, some online sources were used. Any reused or adapted code segments are clearly identified within this notebook. These adaptations stem from various sources, including a Medium article on raincloud plots (Belengeanu, 2022) and an online article on Towards Data Science focusing on SHAP analysis for binary classification (O'Sullivan, 2023). Additionally, ChatGPT has been employed as a debugging tool to rectify coding errors (OpenAI, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17187ce3",
   "metadata": {},
   "source": [
    "## Importing all neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTING ALL NECCESSARY PACKAGES/LIBRARIES\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, make_scorer, confusion_matrix\n",
    "from skopt import BayesSearchCV\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb8d6e",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Consists of:\n",
    "- Deletion of variables too close to target\n",
    "- Deletion of irrelevant variables\n",
    "- Dealing with missing values\n",
    "- Dataset enrichment (creation of train station related variables)\n",
    "- Creating binary target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb69873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DELETING VARIABLES\n",
    "\"\"\"\n",
    "df = pd.read_csv('Datasets/ODiN2022_Databestand.csv',\n",
    "                 delimiter=';', encoding='latin1', na_values='#NULL!')\n",
    "\n",
    "# List of columns dropped\n",
    "columns_to_drop = [\n",
    "    'OP',\n",
    "    'OPID',\n",
    "    'Steekproef',\n",
    "    'Mode',\n",
    "    'Corop',\n",
    "    'BuurtAdam',\n",
    "    'MRA',\n",
    "    'MRDH',\n",
    "    'Utr',\n",
    "    'BouwjaarPa1',\n",
    "    'KBouwjaarPa1',\n",
    "    'KGewichtPa1',\n",
    "    'TenaamPa1',\n",
    "    'BrandstofPa2',\n",
    "    'XBrandstofPa2',\n",
    "    'BrandstofEPa2',\n",
    "    'BouwjaarPa2',\n",
    "    'KBouwjaarPa2',\n",
    "    'KGewichtPa2',\n",
    "    'TenaamPa2',\n",
    "    'BouwjaarPaL',\n",
    "    'KBouwjaarPaL',\n",
    "    'KGewichtPaL',\n",
    "    'Jaar',\n",
    "    'EFiets',\n",
    "    'AutoLPl',\n",
    "    'AutoBed',\n",
    "    'AutoDOrg',\n",
    "    'AutoDPart',\n",
    "    'AutoDBek',\n",
    "    'AutoLeen',\n",
    "    'AutoHuur',\n",
    "    'AutoAnd',\n",
    "    'ByzDag',\n",
    "    'ByzAdr',\n",
    "    'ByzVvm',\n",
    "    'ByzTyd',\n",
    "    'ByzDuur',\n",
    "    'ByzRoute',\n",
    "    'ByzReden',\n",
    "    'Verpl',\n",
    "    'VerplID',\n",
    "    'VerplNr',\n",
    "    'MeerWink',\n",
    "    'VertGeb',\n",
    "    'VertPCBL',\n",
    "    'VertCorop',\n",
    "    'VertMRA',\n",
    "    'VertMRDH',\n",
    "    'VertUtr',\n",
    "    'AankGeb',\n",
    "    'AankPCBL',\n",
    "    'AankCorop',\n",
    "    'AankMRA',\n",
    "    'AankMRDH',\n",
    "    'AankUtr',\n",
    "    'PCG',\n",
    "    'GemG',\n",
    "    'PCBLG',\n",
    "    'HvmRol',\n",
    "    'KHvm',\n",
    "    'VolgWerk',\n",
    "    'SAantAdr',\n",
    "    'SDezPlts',\n",
    "    'SPlaats1',\n",
    "    'SPlaats2',\n",
    "    'SPlaats3',\n",
    "    'SPlaats4',\n",
    "    'SPlaats5',\n",
    "    'AfstS',\n",
    "    'AfstSBL',\n",
    "    'SVvm1',\n",
    "    'SVvm2',\n",
    "    'SVvm3',\n",
    "    'SVvm4',\n",
    "    'SBegUur',\n",
    "    'SBegMin',\n",
    "    'SEindUur',\n",
    "    'SEindMin',\n",
    "    'CorrVerpl',\n",
    "    'GehBLVer',\n",
    "    'Rit',\n",
    "    'RitID',\n",
    "    'RitNr',\n",
    "    'AfstRBL',\n",
    "    'Rvm',\n",
    "    'KRvm',\n",
    "    'RReisduurBL',\n",
    "    'RCorrSnelh',\n",
    "    'RVliegVer',\n",
    "    'FactorH',\n",
    "    'FactorP',\n",
    "    'FactorV'\n",
    "    'Rvm',\n",
    "    'AutoEig',\n",
    "    'AutoHhl',\n",
    "    'AutoLWg',\n",
    "    'WrkVervw',\n",
    "    'RvmRol',\n",
    "    'RAantIn',\n",
    "    'KRvm']\n",
    "\n",
    "# Dropping irrelevant variables/variables that represent target variable directly\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEALING WITH MISSING VALUES\n",
    "\"\"\"\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Number of missing values for each variable:\")\n",
    "for column in df.columns:\n",
    "    print(f\"{column}: {df[column].isnull().sum()}\")\n",
    "\n",
    "# Printing the number of rows and columns remained\n",
    "print(\"Number of rows remained:\", df.shape[0])\n",
    "print(\"Number of columns remainded:\", df.shape[1])\n",
    "\n",
    "# Saving DataFrame to a CSV file\n",
    "df.to_csv('Datasets/participants_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATING VARIABLES TRAIN STATION PROXIMITY, NEAREST TRAINSTATION AND TYPE OF TRAINSTATION\n",
    "\n",
    "Final variables:\n",
    "- nearest_station_encoded: the nearest trainstation to entries homeadress label encoded\n",
    "- station_type_encoded: type of nearest station\n",
    "- distance_to nearest_station_km: distance to nearest station in km (later normalized)\n",
    "\"\"\"\n",
    "\n",
    "# Loading the datasets used for creation of new variables\n",
    "participants_df = pd.read_csv('Datasets/participants_dataset.csv')              # Dataset 1: Participants with postal codes\n",
    "trainstations_df = pd.read_csv('Datasets/trainstations_dataset.csv')            # Dataset 2: Train stations with coordinates\n",
    "postalcodes_df = pd.read_csv('Datasets/postalcodes_dataset.csv', delimiter=';') # Dataset 3: Postal codes with coordinates\n",
    "\n",
    "# Splitting the first column into latitude and longitude and dropping the original column\n",
    "postalcodes_df[['Latitude', 'Longitude']] = postalcodes_df.iloc[:, 0].str.split(',', expand=True)\n",
    "postalcodes_df.drop(postalcodes_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Merging postalcodes_df and participants_df to get participant coordinates\n",
    "participants_with_coordinates = pd.merge(participants_df, postalcodes_df[['PC4', 'Latitude', 'Longitude']], left_on='WoPC', right_on='PC4', how='left')\n",
    "participants_with_coordinates.drop(columns=['PC4'], inplace=True)\n",
    "\n",
    "# Initialising lists to store the nearest train station, the distance and the type of station for each participant\n",
    "nearest_station = []            # Initialising a list to store nearest station\n",
    "distance_to_station = []        # Initialising a list to store distance to nearest station\n",
    "station_type = []               # Initialising a list to store type of nearest station\n",
    "\n",
    "for index, row in participants_with_coordinates.iterrows():     # Iterating over each participant\n",
    "    participant_coords = (row['Latitude'], row['Longitude'])\n",
    "    min_distance = float('inf')\n",
    "    nearest = None\n",
    "    station_type_nearest = None\n",
    "\n",
    "    for _, station_row in trainstations_df.iterrows():          # Iterating over each train station\n",
    "        station_coords = (station_row['geo_lat'], station_row['geo_lng'])\n",
    "        distance = geodesic(participant_coords, station_coords).kilometers\n",
    "\n",
    "        # If current station is closer than the previous closest station, updating min_distance, nearest and station_type_nearest\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest = station_row['name_long']\n",
    "            station_type_nearest = station_row['type']\n",
    "\n",
    "    # Appending the results of each participant to the lists\n",
    "    nearest_station.append(nearest)\n",
    "    distance_to_station.append(min_distance)\n",
    "    station_type.append(station_type_nearest)\n",
    "\n",
    "    # Progress massage for every 1000 participants\n",
    "    if (index + 1) % 1000 == 0:\n",
    "        print(f\"Processed {index + 1} participants...\")\n",
    "\n",
    "# Adding the resulting lists to the participants_with_coordinates df\n",
    "participants_with_coordinates['nearest_station'] = nearest_station\n",
    "participants_with_coordinates['distance_to_station_km'] = distance_to_station\n",
    "participants_with_coordinates['station_type'] = station_type\n",
    "\n",
    "# Label Encoding and dropping original columns\n",
    "label_encoder = LabelEncoder()\n",
    "participants_with_coordinates['nearest_station_encoded'] = label_encoder.fit_transform(participants_with_coordinates['nearest_station'])\n",
    "participants_with_coordinates['station_type_encoded'] = label_encoder.fit_transform(participants_with_coordinates['station_type'])\n",
    "\n",
    "participants_with_coordinates.drop(columns=['nearest_station'], inplace=True)\n",
    "participants_with_coordinates.drop(columns=['station_type'], inplace=True)\n",
    "\n",
    "# Saving the dataframe to csv\n",
    "participants_with_coordinates.to_csv('Datasets/preprocessed_data.csv', index=False)\n",
    "\n",
    "# Progress message\n",
    "print(\"Preprocessed data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05774243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NORMALISING NUMERICAL VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "df_to_normalise = pd.read_csv(\"preprocessed_data.csv\")\n",
    "print(df_to_normalise.head(10))\n",
    "\n",
    "# Initialising the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Specifying the numerical variables that need to be normalised\n",
    "variables_to_normalise = ['AantVpl',\n",
    "                          'AantOVVpl',\n",
    "                          'AantSVpl',\n",
    "                          'ReisduurOP',\n",
    "                          'AfstandOP',\n",
    "                          'AfstandSOP',\n",
    "                          'AfstV',\n",
    "                          'AfstR',\n",
    "                          'VertUur',\n",
    "                          'VertMin',\n",
    "                          'AankUur',\n",
    "                          'AankMin',\n",
    "                          'Reisduur',\n",
    "                          'RVertUur',\n",
    "                          'RVertMin',\n",
    "                          'RAankUur',\n",
    "                          'RAankMin',\n",
    "                          'RReisduur',\n",
    "                          'Latitude',\n",
    "                          'Longitude',\n",
    "                          'distance_to_station_km']\n",
    "\n",
    "# Fitting and transforming the selected variables\n",
    "df_to_normalise[variables_to_normalise] = scaler.fit_transform(df_to_normalise[variables_to_normalise])\n",
    "\n",
    "# Displaying the normalised dataframe\n",
    "print(df_to_normalise.head(10))\n",
    "\n",
    "# Saving normalised dataframe\n",
    "df_to_normalise.to_csv('Datasets/normalised_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATING BINARY TARGET VARIABLE\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('Datasets/normalised_dataset.csv')\n",
    "\n",
    "# Specifying the labels for sustainable and not sustainable transportation modes\n",
    "sustainable_labels = {\n",
    "    2: 'Trein',\n",
    "    3: 'Bus',\n",
    "    4: 'Tram',\n",
    "    5: 'Metro',\n",
    "    6: 'Speedpedelec',\n",
    "    7: 'Elektrische fiets',\n",
    "    8: 'Niet-elektrische fiets',\n",
    "    9: 'Te voet',\n",
    "    19: 'Gehandicaptenvervoermiddel met motor',\n",
    "    20: 'Gehandicaptenvervoermiddel zonder motor',\n",
    "    21: 'Skates/skeelers/step',\n",
    "    24: 'Anders zonder motor'\n",
    "}\n",
    "\n",
    "not_sustainable_labels = {\n",
    "    1: 'Personenauto',\n",
    "    10: 'Touringcar',\n",
    "    11: 'Bestelauto',\n",
    "    12: 'Vrachtwagen',\n",
    "    13: 'Camper',\n",
    "    14: 'Taxi/Taxibusje',\n",
    "    15: 'Landbouwvoertuig',\n",
    "    16: 'Motor',\n",
    "    17: 'Bromfiets',\n",
    "    18: 'Snorfiets',\n",
    "    22: 'Boot',\n",
    "    23: 'Anders met motor',\n",
    "}\n",
    "# Categorise the transportation modes into binary target variable\n",
    "df['category'] = df['Hvm'].map(lambda x: 'Sustainable' if x in sustainable_labels else 'Not Sustainable')\n",
    "\n",
    "# Encoding the categories\n",
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['category'])\n",
    "df.drop(columns=['category'], inplace=True)\n",
    "\n",
    "print(df.head(5))\n",
    "print(df.shape)\n",
    "\n",
    "# Saving final preprocessed dataset\n",
    "df.to_csv('Datasets/final_preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac582099",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Consists of:\n",
    "- Pie chart HVM\n",
    "- Pie charts per Gender\n",
    "- Pie chart binary target variable\n",
    "- Raincloud plot age/target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f454fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pie chart hoofdvervoermiddel\n",
    "\"\"\"\n",
    "df = pd.read_csv('Datasets/final_preprocessed_data.csv')\n",
    "\n",
    "# Converting 'Hvm' column to numeric and counting occurences\n",
    "df['Hvm'] = pd.to_numeric(df['Hvm'], errors='coerce')\n",
    "vervoer_soorten = df['Hvm'].value_counts()\n",
    "\n",
    "# Selecting the top 5 and combining the rest into a category called 'Other' (for the sake of readability)\n",
    "top_5 = vervoer_soorten.head(5)\n",
    "other_transportation_modes = vervoer_soorten[~vervoer_soorten.index.isin(top_5.index)].sum()\n",
    "vervoer_soorten_combined = pd.concat([top_5, pd.Series({'Other': other_transportation_modes.sum()})])\n",
    "\n",
    "# Specifying the labels\n",
    "labels = {\n",
    "    1: 'Personenauto',\n",
    "    2: 'Trein',\n",
    "    3: 'Bus',\n",
    "    4: 'Tram',\n",
    "    5: 'Metro',\n",
    "    6: 'Speedpedelec',\n",
    "    7: 'Elektrische fiets',\n",
    "    8: 'Niet-elektrische fiets',\n",
    "    9: 'Te voet',\n",
    "    10: 'Touringcar',\n",
    "    11: 'Bestelauto',\n",
    "    12: 'Vrachtwagen',\n",
    "    13: 'Camper',\n",
    "    14: 'Taxi/Taxibusje',\n",
    "    15: 'Landbouwvoertuig',\n",
    "    16: 'Motor',\n",
    "    17: 'Bromfiets',\n",
    "    18: 'Snorfiets',\n",
    "    19: 'Gehandicaptenvervoermiddel met motor',\n",
    "    20: 'Gehandicaptenvervoermiddel zonder motor',\n",
    "    21: 'Skates/skeelers/step',\n",
    "    22: 'Boot',\n",
    "    23: 'Anders met motor',\n",
    "    24: 'Anders zonder motor'\n",
    "}\n",
    "labels_for_pie = vervoer_soorten_combined.index.map(lambda x: labels[x] if x in labels else 'Other')\n",
    "\n",
    "# Setting colours for the pie chart\n",
    "colours = ['#FDE725FF', '#95D840FF', '#55C667FF', '#29AF7FFF', '#238A8DFF', '#39568CFF']\n",
    "\n",
    "# Plot the pie chart with custom labels\n",
    "plt.pie(vervoer_soorten_combined, labels=labels_for_pie, colors=colours, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Main Mode of Transport')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/distribution_main_mode_of_transport.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pie charts per gender\n",
    "\"\"\"\n",
    "df = pd.read_csv('Datasets/final_preprocessed_data.csv')\n",
    "\n",
    "# Filtering data for males and females\n",
    "males_data = df[df['Geslacht'] == 1]\n",
    "females_data = df[df['Geslacht'] == 2]\n",
    "\n",
    "# Calculating sustainable and not sustainable transportation percentages\n",
    "males_counts = males_data['category_encoded'].value_counts(normalize=True) * 100\n",
    "females_counts = females_data['category_encoded'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Creating a figure with two subplots (pie charts per gender)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Pie chart males\n",
    "axes[0].pie(males_counts, labels=['Not Sustainable', 'Sustainable'],\n",
    "            autopct='%1.1f%%', colors=['#FDE725FF', '#20A387FF'], startangle=90)\n",
    "axes[0].set_title('Males')\n",
    "\n",
    "# Pie chart females\n",
    "axes[1].pie(females_counts, labels=['Not Sustainable', 'Sustainable'],\n",
    "            autopct='%1.1f%%', colors=['#FDE725FF', '#20A387FF'], startangle=90)\n",
    "axes[1].set_title('Females')\n",
    "\n",
    "fig.suptitle('Transportation Distribution by Gender', fontsize=16)\n",
    "plt.savefig('Plots/transportation_distribution_by_gender.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pie Chart binary target variable\n",
    "\"\"\"\n",
    "df = pd.read_csv('Datasets/final_preprocessed_data.csv')\n",
    "\n",
    "# Converting 'Hvm' column to numeric and counting occurences\n",
    "df['category_encoded'] = pd.to_numeric(df['category_encoded'], errors='coerce')\n",
    "vervoer_soorten = df['category_encoded'].value_counts()\n",
    "\n",
    "# Specifying labels\n",
    "labels = {\n",
    "    0: 'Not Sustainable \\n Transportation',\n",
    "    1: 'Sustainable \\n Transportation',\n",
    "}\n",
    "\n",
    "labels_for_pie = vervoer_soorten.index.map(lambda x: labels.get(x, 'Unknown'))\n",
    "\n",
    "# Setting properties\n",
    "colors = ['#20A387FF', '#FDE725FF']\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "\n",
    "# Pie chart binary target variable\n",
    "plt.pie(vervoer_soorten, labels=labels_for_pie, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/distribution_sustainable_not_sustainable.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb810f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom Plot to create raincloud plot\n",
    "\n",
    "    Input:\n",
    "        data: list of 1D numpy arrays containing feature values\n",
    "        names: list of length 2 with feature names\n",
    "        (optional) max_scatters: maximum number of scatter points at each unique feature\n",
    "    Output:\n",
    "        fig: figure object, use e.g. plt.show() and plt.savefig after retrieving this function output\n",
    "    \n",
    "    Adapted from: https://medium.com/@alexbelengeanu/getting-started-with-raincloud-plots-in-python-2ea5c2d01c11\n",
    "\"\"\"\n",
    "\n",
    "def make_it_rain(data,names,max_scatters=10):\n",
    "\n",
    "    # Setting font properties for the entire plot\n",
    "    plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "    \n",
    "    # Creating the figure\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "    # Boxplot\n",
    "    bp = ax.boxplot(data, patch_artist = True, vert = False)\n",
    "\n",
    "    boxplots_colors = ['#FDE725FF', '#FDE725FF']\n",
    "    for patch, color in zip(bp['boxes'], boxplots_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.4)\n",
    "\n",
    "    # Violinplot\n",
    "    vp = ax.violinplot(data, points=500, \n",
    "                showmeans=False, showextrema=False, showmedians=False, vert=False)\n",
    "    \n",
    "    violin_colors = ['#29AF7FFF', '#29AF7FFF']\n",
    "    for idx, b in enumerate(vp['bodies']):\n",
    "        # Get the center of the plot\n",
    "        m = np.mean(b.get_paths()[0].vertices[:, 0])\n",
    "        # Modifying to only see the upper half of the violin plot\n",
    "        b.get_paths()[0].vertices[:, 1] = np.clip(b.get_paths()[0].vertices[:, 1], idx+1, idx+2)\n",
    "        b.set_color(violin_colors[idx])\n",
    "\n",
    "    # Scatter plot\n",
    "    scatter_colors = ['#39568CFF', '#39568CFF']\n",
    "\n",
    "    for idx, feature in enumerate(data):\n",
    "        # Only take a portion of the data to make the data not too dense\n",
    "        feature_uniques, feature_counts = np.unique(feature, return_counts=True)\n",
    "        feature_counts_dict = dict(zip(feature_uniques, feature_counts))\n",
    "        feature_counts_capped = np.empty((0,0), int)\n",
    "        for feature_unique_current in feature_uniques:\n",
    "            feature_count_current = feature_counts_dict[feature_unique_current]\n",
    "            if feature_count_current > max_scatters:\n",
    "                feature_counts_capped = np.append(feature_counts_capped,np.full(max_scatters,feature_unique_current))\n",
    "            else:\n",
    "                feature_counts_capped = np.append(feature_counts_capped,np.full(feature_count_current,feature_unique_current))\n",
    "        \n",
    "        # Creating initial scatter positions without randomisation\n",
    "        scatter_height = np.full(len(feature_counts_capped), idx + .8).astype(float)\n",
    "        # Adding random jitter effect so the features do not overlap on the y-axis\n",
    "        feature_capped_idxs = np.arange(len(scatter_height))\n",
    "        scatter_height.flat[feature_capped_idxs] += np.random.uniform(low=-.05, high=.05, size=len(feature_capped_idxs))\n",
    "        # Draw scatters\n",
    "        plt.scatter(feature_counts_capped, scatter_height, s=.3, c=scatter_colors[idx])\n",
    "\n",
    "    plt.yticks(np.arange(1,3,1), names, fontsize=16)  # Set text labels\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Raincloud plot Age and Target Variable\n",
    "\"\"\"\n",
    "df = pd.read_csv('Datasets/final_preprocessed_data.csv')\n",
    "\n",
    "# Filtering the data per category and than combining into one variable\n",
    "age_sustainable = df[df['category_encoded'] == 1]['Leeftijd'].to_numpy()\n",
    "age_not_sustainable = df[df['category_encoded'] == 0]['Leeftijd'].to_numpy()\n",
    "age_data = [age_not_sustainable,age_sustainable]\n",
    "\n",
    "# Set figure metadata\n",
    "feature_names = [\"Not sustainable\",\"Sustainable\"]\n",
    "max_scatters = 10\n",
    "\n",
    "# Make it rain\n",
    "fig = cp.make_it_rain(age_data,feature_names,max_scatters)\n",
    "\n",
    "plt.savefig('Plots/raincloud_age_category.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d0990",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037368ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Datasets/final_preprocessed_data.csv')\n",
    "\n",
    "# Drop 'Hvm'column\n",
    "data.drop('Hvm', axis=1, inplace=True)\n",
    "print(\"Data shape after dropping 'Hvm':\", data.shape)\n",
    "\n",
    "# Splitting the data in X and y\n",
    "y = data['category_encoded']\n",
    "X = data.drop('category_encoded', axis=1)\n",
    "\n",
    "# Splittingthe data into training/validation and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Saving the data splits into separate CSV files\n",
    "X_train.to_csv('Datasets/X_train.csv', index=False)\n",
    "y_train.to_csv('Datasets/y_train.csv', index=False)\n",
    "X_test.to_csv('Datasets/X_test.csv', index=False)\n",
    "y_test.to_csv('Datasets/y_test.csv', index=False)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b31ad",
   "metadata": {},
   "source": [
    "### Training the initial models\n",
    "\n",
    "- Default Decision Tree\n",
    "- Default Random Forest\n",
    "- Deafault Gradient Boosting\n",
    "- Default ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default Decision Tree\n",
    "\"\"\"\n",
    "X_trainval = pd.read_csv('Datasets/X_train.csv')\n",
    "y_trainval = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "# Extra split to create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=42)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_val)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "print(f\"Balanced Accuracy Default Decision Tree: {balanced_accuracy}\")\n",
    "\n",
    "precision = precision_score(y_val, y_pred, average='macro')\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "recall = recall_score(y_val, y_pred, average='macro')\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default Random Forest\n",
    "\"\"\"\n",
    "\n",
    "X_trainval = pd.read_csv('Datasets/X_train.csv')\n",
    "y_trainval = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "# Extra split to create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_val)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "print(f\"Balanced Accuracy Default Random Forest: {balanced_accuracy}\")\n",
    "\n",
    "precision = precision_score(y_val, y_pred, average='macro')\n",
    "print(f\"Precision Default Random Forest: {precision}\")\n",
    "\n",
    "recall = recall_score(y_val, y_pred, average='macro')\n",
    "print(f\"Recall Default Random Forest: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default Gradient Boosting\n",
    "\"\"\"\n",
    "\n",
    "X_trainval = pd.read_csv('Datasets/X_train.csv')\n",
    "y_trainval = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "# Extra split to create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=42)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gradient_boosting.fit(X_train, y_train.values.ravel())\n",
    "y_pred = gradient_boosting.predict(X_val)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "print(f\"Balanced Accuracy Default Gradient Boosting Classifier: {balanced_accuracy}\")\n",
    "\n",
    "precision = precision_score(y_val, y_pred, average='macro')\n",
    "print(f\"Precision Deafault Gradient Boosting Classifier: {precision}\")\n",
    "\n",
    "recall = recall_score(y_val, y_pred, average='macro')\n",
    "print(f\"Recall Default Gradient Boosting Classifier: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default ANN\n",
    "\"\"\"\n",
    "\n",
    "X_trainval = pd.read_csv('Datasets/X_train.csv')\n",
    "y_trainval = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "# Extra split to create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=42)\n",
    "\n",
    "# Convert y_train to numpy array and flatten for binary classification\n",
    "y_train = np.array(y_train).ravel()\n",
    "\n",
    "# Function creating default ANN model\n",
    "def create_ann_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n",
    "\n",
    "# ANN Classifier\n",
    "model = create_ann_model()\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "y_pred_proba = model.predict(X_val_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).ravel()  # Prob to binary (0/1)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_val, y_pred)\n",
    "print(f\"Balanced Accuracy Default ANN: {balanced_accuracy}\")\n",
    "\n",
    "precision = precision_score(y_val, y_pred, average='macro')\n",
    "print(f\"Precision Deafault ANN: {precision}\")\n",
    "\n",
    "recall = recall_score(y_val, y_pred, average='macro')\n",
    "print(f\"Recall Default ANN: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f77532",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimisation of Decision Tree Classifier\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "# Search Space Hyperparameters\n",
    "search_space_hyperparameters = {\n",
    "    'max_depth': (1, 150),              # Maximum depth of the tree\n",
    "    'min_samples_split': (2, 50),       # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': (1, 100),       # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "\n",
    "# Bayesian Optimisation of Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "bayesian_opt = BayesSearchCV(\n",
    "    estimator=decision_tree,\n",
    "    search_spaces=search_space_hyperparameters,\n",
    "    n_iter=50,                        # Number of parameter settings that are sampled\n",
    "    cv=skf,                            # Number of cross-validation folds\n",
    "    n_jobs=-1,                         # Use all available CPU cores\n",
    "    scoring=scorer,                    # Metric to optimize\n",
    "    random_state=42,                   # Random seed for reproducibility\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "bayesian_opt.fit(X_train, y_train)\n",
    "\n",
    "# Printing the best hyperparameters and the corresponding metrics\n",
    "print(\"Best hyperparameters Decision Tree:\")\n",
    "print(bayesian_opt.best_params_)\n",
    "print(\"Best balanced accuracy Decision Tree:\", bayesian_opt.best_score_)\n",
    "best_hyperparameters = bayesian_opt.best_params_\n",
    "\n",
    "# Storing the best hyperparameters\n",
    "with open('best_hyperparameters_decision_tree.json', 'w') as f:\n",
    "    json.dump(best_hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix and Evaluation Metrics of Final Decision Tree\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "X_test = pd.read_csv('Datasets/X_test.csv')\n",
    "y_test = pd.read_csv('Datasets/y_test.csv')\n",
    "\n",
    "with open('best_hyperparameters_decision_tree.json', 'r') as f:\n",
    "    loaded_best_hyperparameters = json.load(f)\n",
    "\n",
    "# Final Decision Tree\n",
    "best_decision_tree = DecisionTreeClassifier(**loaded_best_hyperparameters, random_state=42)\n",
    "best_decision_tree.fit(X_train, y_train.values.ravel())\n",
    "y_pred = best_decision_tree.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "balanced_accuracy_loaded = balanced_accuracy_score(y_test, y_pred)\n",
    "recall_loaded = recall_score(y_test, y_pred, average='weighted')\n",
    "precision_loaded = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Performance on the test set with loaded best hyperparameters:\")\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_loaded)\n",
    "print(\"Recall:\", recall_loaded)\n",
    "print(\"Precision:\", precision_loaded)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Setting font properties\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('Plots/confusion_matrix_decision_tree.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98593105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimisation of Random Forest Classifier\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "\n",
    "# Search Space Hyperparameters\n",
    "search_space_hyperparameters = {\n",
    "    'n_estimators': (2, 500),\n",
    "    'max_depth': (1, 150),\n",
    "    'min_samples_split': (2, 20),\n",
    "    'min_samples_leaf': (1, 20)\n",
    "}\n",
    "\n",
    "# Bayesian Optimisation of Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "bayesian_search = BayesSearchCV(\n",
    "    estimator=random_forest,\n",
    "    search_spaces=search_space_hyperparameters,\n",
    "    n_iter=50,                         # Number of parameter settings that are sampled\n",
    "    cv=skf,                            # Number of cross-validation folds\n",
    "    n_jobs=-1,                         # Use all available CPU cores\n",
    "    scoring=scorer,                    # Metric to optimize\n",
    "    random_state=42,                   # Random seed for reproducibility\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "bayesian_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Printing the best hyperparameters and the corresponding metrics\n",
    "print(\"Best hyperparameters for Random Forest found using Bayesian optimization:\")\n",
    "print(bayesian_search.best_params_)\n",
    "print(\"Best cross-validation score Random Forest using Bayesian optimization:\", bayesian_search.best_score_)\n",
    "best_hyperparameters = bayesian_search.best_params_\n",
    "\n",
    "# Storing the best hyperparameters\n",
    "with open('best_hyperparameters_random_forest.json', 'w') as f:\n",
    "    json.dump(best_hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5463d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix and Evaluation Metrics of Final Random Forest\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "X_test = pd.read_csv('Datasets/X_test.csv')\n",
    "y_test = pd.read_csv('Datasets/y_test.csv')\n",
    "\n",
    "with open('best_hyperparameters_random_forest.json', 'r') as f:\n",
    "    best_hyperparameters = json.load(f)\n",
    "\n",
    "# Final Random Forest\n",
    "best_random_forest = RandomForestClassifier(**best_hyperparameters, random_state=42)\n",
    "best_random_forest.fit(X_train, y_train.values.ravel())\n",
    "y_pred = best_random_forest.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Performance on the test set:\")\n",
    "print(\"Balanced Accuracy Random Forest:\", balanced_accuracy)\n",
    "print(\"Recall Random Forest:\", recall)\n",
    "print(\"Precision Random Forest:\", precision)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Setting Font Properties\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('Plots/confusion_matrix_random_forest.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimisation of Gradient Boosting Classifier\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "# Search Space Hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [7, 9, 11],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# Grid Search for gradient boosting\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid, cv=skf, scoring=scorer, verbose=2)\n",
    "grid_search.fit(X_train, y_train.values.ravel())  # ravel y_train to avoid DataConversionWarning\n",
    "\n",
    "# Evaluation Metrics\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Balanced Accuracy:\", best_accuracy)\n",
    "\n",
    "# Storing the best hyperparameters\n",
    "with open('best_hyperparameters_gradient_boosting.json', 'w') as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix and Evaluation Metrics of Final Gradient Boosting\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "X_test = pd.read_csv('Datasets/X_test.csv')\n",
    "y_test = pd.read_csv('Datasets/y_test.csv')\n",
    "\n",
    "with open('best_hyperparameters_gradient_boosting.json', 'r') as f:\n",
    "    loaded_best_hyperparameters = json.load(f)\n",
    "\n",
    "# Final Gradient Boosting\n",
    "best_random_forest = GradientBoostingClassifier(**loaded_best_hyperparameters, random_state=42)\n",
    "best_random_forest.fit(X_train, y_train.values.ravel())\n",
    "y_pred = best_random_forest.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Performance on the test set:\")\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Set font properties\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('Plots/confusion_matrix_gradient_boosting.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebf7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimalisation of ANN\n",
    "\n",
    "Automatically tuned on param grid, and manually tuned number of epochs and batch size\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "# Converting y_train to numpy array and flatten for binary classification\n",
    "y_train = np.array(y_train).ravel()\n",
    "\n",
    "# Initialise Stratified K-Fold with k=10\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to create ANN model with dropout and variable number of layers\n",
    "def create_ann_model(hidden_layer_size=64, learning_rate=0.001, dropout_rate=0.0, num_layers=2,\n",
    "                     activation_function='relu', regularization=None, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, activation=activation_function, input_shape=(X_train.shape[1],),\n",
    "                    kernel_regularizer=regularization))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for _ in range(num_layers - 1):  # Add additional hidden layers\n",
    "        model.add(Dense(hidden_layer_size, activation=activation_function, kernel_regularizer=regularization))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Search Space Hyperparameters\n",
    "param_grid = {\n",
    "    'hidden_layer_size': [32],\n",
    "    'learning_rate': [0.01],\n",
    "    'dropout_rate': [0.1],\n",
    "    'num_layers': [2, 4],\n",
    "    'activation_function': ['relu', 'sigmoid'],\n",
    "    'regularization': [None, 'l1'],\n",
    "    'optimizer': ['adam']\n",
    "}\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "# Grid search ANN\n",
    "for params in ParameterGrid(param_grid):\n",
    "    fold_accuracy = []\n",
    "    print(\"Current parameters:\", params)\n",
    "    \n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Standardize the input features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "        \n",
    "        # Training the ANN model\n",
    "        model = create_ann_model(hidden_layer_size=params['hidden_layer_size'],\n",
    "                                 learning_rate=params['learning_rate'],\n",
    "                                 dropout_rate=params['dropout_rate'],\n",
    "                                 num_layers=params['num_layers'],\n",
    "                                 activation_function=params['activation_function'],\n",
    "                                 regularization=params['regularization'],\n",
    "                                 optimizer=params['optimizer'])\n",
    "        model.fit(X_train_fold_scaled, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
    "        \n",
    "        # Evaluating the model on the validation fold\n",
    "        y_pred_proba = model.predict(X_val_fold_scaled)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).ravel()  # Prob to binary (0/1)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_val_fold, y_pred)\n",
    "        fold_accuracy.append(balanced_accuracy)\n",
    "    \n",
    "    # Calculating the mean accuracy over all folds\n",
    "    mean_accuracy = np.mean(fold_accuracy)\n",
    "    print(f\"Mean Accuracy for current parameters: {mean_accuracy}\")\n",
    "    \n",
    "    # Updating best parameters if current parameters result in better accuracy\n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_params = params\n",
    "\n",
    "# Evaluation Metrics\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best accuracy:\", best_accuracy)\n",
    "\n",
    "# Store the best hyperparameters\n",
    "with open('best_hyperparameters_ann.json', 'w') as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0fc706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix and Evaluation Metrics of Final ANN\n",
    "\"\"\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "X_test = pd.read_csv('Datasets/X_test.csv')\n",
    "y_test = pd.read_csv('Datasets/y_test.csv')\n",
    "\n",
    "\n",
    "# Convert y_train to numpy array and flatten for binary classification\n",
    "y_train = np.array(y_train).ravel()\n",
    "\n",
    "# Function to manually tune ANN to best hyperparametersettings\n",
    "def create_ann_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "model = create_ann_model()\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).ravel()  # Prob to binary (0/1)\n",
    "\n",
    "# Evaluation Metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Balanced Accuracy Default ANN: {balanced_accuracy}\")\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "print(f\"Precision Default ANN: {precision}\")\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(f\"Recall Default ANN: {recall}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Set font properties\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('Plots/confusion_matrix_ann.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae71104",
   "metadata": {},
   "source": [
    "### Error Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "X_test = pd.read_csv('Datasets/X_test.csv')\n",
    "y_test = pd.read_csv('Datasets/y_test.csv')\n",
    "\n",
    "with open('best_hyperparameters_gradient_boosting.json', 'r') as f:\n",
    "    loaded_best_hyperparameters = json.load(f)\n",
    "\n",
    "# Train and fit tuned gradient boosting classifier\n",
    "best_gradient_boosting = GradientBoostingClassifier(**loaded_best_hyperparameters, random_state=42)\n",
    "best_gradient_boosting.fit(X_train, y_train.values.ravel())\n",
    "y_pred = best_gradient_boosting.predict(X_test)\n",
    "\n",
    "# Calculate overall evaluation metrics\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Separating the data by gender\n",
    "male_indices = X_test[X_test['Geslacht'] == 1].index\n",
    "female_indices = X_test[X_test['Geslacht'] == 2].index\n",
    "\n",
    "X_test_male = X_test.loc[male_indices].drop(columns=['Geslacht'])\n",
    "y_test_male = y_test.loc[male_indices]\n",
    "y_pred_male = y_pred[male_indices]\n",
    "\n",
    "X_test_female = X_test.loc[female_indices].drop(columns=['Geslacht'])\n",
    "y_test_female = y_test.loc[female_indices]\n",
    "y_pred_female = y_pred[female_indices]\n",
    "\n",
    "# Evaluation metrics for males\n",
    "balanced_accuracy_male = balanced_accuracy_score(y_test_male, y_pred_male)\n",
    "recall_male = recall_score(y_test_male, y_pred_male, average='weighted')\n",
    "precision_male = precision_score(y_test_male, y_pred_male, average='weighted')\n",
    "\n",
    "print(\"Performance for Males on the test set:\")\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_male)\n",
    "print(\"Recall:\", recall_male)\n",
    "print(\"Precision:\", precision_male)\n",
    "\n",
    "# Confusion matrix for males\n",
    "conf_matrix_male = confusion_matrix(y_test_male, y_pred_male)\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix_male, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Males')\n",
    "plt.savefig('Plots/confusion_matrix_gradient_boosting_males.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Evaluation metrics for females\n",
    "balanced_accuracy_female = balanced_accuracy_score(y_test_female, y_pred_female)\n",
    "recall_female = recall_score(y_test_female, y_pred_female, average='weighted')\n",
    "precision_female = precision_score(y_test_female, y_pred_female, average='weighted')\n",
    "\n",
    "print(\"Performance for Females on the test set:\")\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_female)\n",
    "print(\"Recall:\", recall_female)\n",
    "print(\"Precision:\", precision_female)\n",
    "\n",
    "# Confusion matrix for females\n",
    "conf_matrix_female = confusion_matrix(y_test_female, y_pred_female)\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.size': 16})\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix_female, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Females')\n",
    "plt.savefig('Plots/confusion_matrix_gradient_boosting_females.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b93db",
   "metadata": {},
   "source": [
    "### Exploration of Key Determinants\n",
    "\n",
    "Consists of:\n",
    "- Feature Importance Analysis\n",
    "- Shap Analysis\n",
    "- Category Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c23c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Importance Analysis\n",
    "\"\"\"\n",
    "\n",
    "with open('best_hyperparameters_gradient_boosting.json', 'r') as file:\n",
    "    best_hyperparameters_dict = json.load(file)\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "X_train.rename(columns={'distance_to_station_km': 'Dist_station', 'nearest_station_encoded': 'Nearest_station'}, inplace=True)\n",
    "\n",
    "# Final Gradient Boosting as this is best performing model\n",
    "best_gradient_boosting = GradientBoostingClassifier(random_state=42, **best_hyperparameters_dict)\n",
    "best_gradient_boosting.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculating Feature Importances\n",
    "feature_importance_scores = best_gradient_boosting.feature_importances_\n",
    "feature_importance_dict = dict(zip(X_train.columns, feature_importance_scores))\n",
    "\n",
    "# Creating a DataFrame to store feature names and importance scores\n",
    "feature_importance_series = pd.Series(feature_importance_scores)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance Score': feature_importance_series\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance Score', ascending=False)\n",
    "feature_names = feature_importance_df['Feature'][:20]\n",
    "importance_scores = feature_importance_df['Importance Score'][:20]\n",
    "\n",
    "# Define colours\n",
    "num_colours = 20\n",
    "viridis_cmap = plt.cm.get_cmap('viridis')\n",
    "colors = [viridis_cmap(i / num_colours) for i in range(num_colours)]\n",
    "\n",
    "# Plot feature importance scores\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.grid(alpha=0.7, linestyle='dotted', zorder=0)\n",
    "bars = plt.barh(feature_names, importance_scores, color=colors, zorder=3)\n",
    "plt.xlabel('Importance Score', fontsize=22, fontname='Times New Roman')\n",
    "plt.ylabel('Feature', fontsize=22, fontname='Times New Roman')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(fontsize=22, fontname='Times New Roman')\n",
    "plt.yticks(fontsize=22, fontname='Times New Roman')\n",
    "\n",
    "# Adding score next to each bar\n",
    "for bar, score in zip(bars, importance_scores):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{score:.3f}', \n",
    "             ha='left', va='center', color='black', fontsize=22, fontname='Times New Roman')\n",
    "# Enlarging the x-axis\n",
    "plt.xlim(right=max(importance_scores) * 1.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/feature_importance.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SHAP Analysis\n",
    "\n",
    "Adapted from: https://towardsdatascience.com/shap-for-binary-and-multiclass-target-variables-ff2f43de0cf4\n",
    "\"\"\"\n",
    "\n",
    "with open('best_hyperparameters_gradient_boosting.json', 'r') as file:\n",
    "    best_hyperparameters_dict = json.load(file)\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "X_test = pd.read_csv('Datasets/X_test.csv')\n",
    "y_test = pd.read_csv('Datasets/y_test.csv')\n",
    "\n",
    "# Final Gradient Boosting as this is best performing model\n",
    "best_gradient_boosting = GradientBoostingClassifier(random_state=42, **best_hyperparameters_dict)\n",
    "best_gradient_boosting.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Creating a SHAP values\n",
    "explainer = shap.TreeExplainer(best_gradient_boosting)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Storing SHAP values\n",
    "with open('shap_values.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values, f)\n",
    "with open('shap_values.pkl', 'rb') as f:\n",
    "    loaded_shap_values = pickle.load(f)\n",
    "\n",
    "# Converting the NumPy array to an Explanation object\n",
    "shap_values_explanation = shap.Explanation(values=loaded_shap_values, feature_names=feature_names)\n",
    "\n",
    "# Getting column names from the training data\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Setting font properties\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', \n",
    "                     'font.size': 24})\n",
    "\n",
    "# Plot SHAP values\n",
    "shap.plots.bar(shap_values_explanation[0], max_display=20, show=False)\n",
    "\n",
    "plt.xlabel(\"SHAP Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/barplot_shap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Category Importance Analysis\n",
    "\"\"\"\n",
    "\n",
    "X_train = pd.read_csv('Datasets/X_train.csv')\n",
    "y_train = pd.read_csv('Datasets/y_train.csv')\n",
    "\n",
    "with open('Datasets/feature_importance_scores.json', 'r') as file:\n",
    "    feature_importance_scores = json.load(file)\n",
    "\n",
    "# Category Mappings\n",
    "category_mappings = {\n",
    "    'Trip Information': ['BerWrk', 'RdWrkA', 'RdWrkB', 'BerOnd', 'RdOndA', 'RdOndB', \n",
    "                         'BerSup', 'RdSupA', 'RdSupB', 'BerZiek', 'RdZiekA', 'RdZiekB', 'BerArts', \n",
    "                         'RdArtsA', 'RdArtsB', 'BerStat', 'RdStatA', 'RdStatB', 'BerHalte', \n",
    "                         'RdHalteA', 'RdHalteB', 'BerFam', 'RdFamA', 'RdFamB', 'BerSport', \n",
    "                         'RdSportA', 'RdSportB', 'Weggeweest', 'RedenNW', 'RedenNWZ', 'RedenNWB', \n",
    "                         'RedenNWW', 'AantVpl', 'AantOVVpl', 'AantSVpl',\n",
    "                         'ReisduurOP', 'AfstandOP', 'AfstandSOP', 'Doel', 'MotiefV', \n",
    "                         'KMotiefV', 'AardWerk', 'VertLoc', 'AfstV', 'KAfstV', 'AfstR', \n",
    "                         'KAfstR', 'RTSamen', 'AantRit', 'Toer'],\n",
    "    'Regional Information': ['WoPC', 'WoGem', 'Sted', 'GemGr', 'Prov', 'VertPC', 'VertGem', \n",
    "                             'VertProv', 'AankPC', 'AankGem', 'AankProv', 'RVertStat', 'RAankStat', \n",
    "                             'Longitude', 'Latitude', 'distance_to_station_km', 'station_type_encoded',\n",
    "                             'nearest_station_encoded'],\n",
    "    'Financial Information': ['BetWerk', 'OnbBez', 'HHBestInkG', 'HHGestInkG', 'HHLaagInk', \n",
    "                               'HHSocInk', 'HHWelvG', 'OVStKaart', 'WrkVerg', 'VergVast', 'VergKm', \n",
    "                               'VergBrSt', 'VergOV', 'VergAans', 'VergVoer', 'VergBudg', 'VergPark', \n",
    "                               'VergStal', 'VergAnd'],\n",
    "    'Demographic Information': ['HHPers', 'HHSam', 'HHPlOP', 'HHLft1', 'HHLft2', 'HHLft3', 'HHLft4', \n",
    "                                 'Geslacht', 'Leeftijd', 'KLeeft', 'Herkomst', 'MaatsPart', 'Opleiding', \n",
    "                                 'HHRijbewijsAu', 'HHRijbewijsMo', 'HHRijbewijsBr', 'OPRijbewijsAu', \n",
    "                                 'OPRijbewijsMo', 'OPRijbewijsBr', 'HHAuto', 'HHAutoL', 'OPAuto', \n",
    "                                 'BrandstofPa1', 'XBrandstofPaL', 'XBrandstofPa1', 'BrandstofEPaL', 'BrandstofEPa1', 'BrandstofPaL', \n",
    "                                 'HHMotor', 'OPMotor', 'HHBrom', 'OPBrom', 'HHSnor', 'OPSnor', \n",
    "                                 'HHEFiets', 'HHBezitVm', 'OPBezitVm', 'FqLopen', 'FqNEFiets', \n",
    "                                 'FqEFiets', 'FqBTM', 'FqTrein', 'FqAutoB', 'FqAutoP', 'FqMotor', \n",
    "                                 'FqBrSnor', 'Kind6'],\n",
    "    'Temporal Dynamics': ['Maand', 'Week', 'Dag', 'Weekdag', 'Feestdag', 'VertUur', 'VertMin',\n",
    "                          'KVertTijd', 'AankUur', 'AankMin', 'Reisduur', 'KReisduur', 'RVertUur', \n",
    "                          'RVertMin', 'RAankUur', 'RAankMin', 'RReisduur']\n",
    "}\n",
    "\n",
    "# Check if all features are mapped\n",
    "all_features = set(X_train.columns)\n",
    "mapped_features = set(feature for features in category_mappings.values() for feature in features)\n",
    "unmapped_features = all_features - mapped_features\n",
    "if unmapped_features:\n",
    "    print(\"Error: The following variables are not mapped to any category:\", ', '.join(unmapped_features))\n",
    "\n",
    "\n",
    "# Check if all mapped features are present in the dataset\n",
    "missing_features = mapped_features - all_features\n",
    "if missing_features:\n",
    "    print(\"Error: The following mapped variables are not present in the dataset:\", ', '.join(missing_features))\n",
    "\n",
    "# Category Importance score\n",
    "category_importance_scores = {}\n",
    "\n",
    "for category, features in category_mappings.items():\n",
    "    category_importance_scores[category] = sum([feature_importance_scores[feature] for feature in features])\n",
    "\n",
    "    sorted_category_importance_scores = dict(sorted(category_importance_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print category importance scores\n",
    "print(\"Category Importance Scores:\")\n",
    "for category, importance_score in sorted_category_importance_scores.items():\n",
    "    print(f\"{category}: {importance_score:.4f}\")\n",
    "\n",
    "# Identifying the most influential feature for each category\n",
    "most_influential_features = {}\n",
    "for category, features in category_mappings.items():\n",
    "    feature_scores = {feature: feature_importance_scores[feature] for feature in features}\n",
    "    most_influential_features[category] = max(feature_scores, key=feature_scores.get)\n",
    "\n",
    "# Print the most influential feature for each category\n",
    "print(\"\\nMost Influential Features from Each Category:\")\n",
    "for category, feature in most_influential_features.items():\n",
    "    print(f\"{category}: {feature} ({feature_importance_scores[feature]:.4f})\")\n",
    "\n",
    "# Category names and their importance scores\n",
    "category_names = list(sorted_category_importance_scores.keys())\n",
    "category_scores = list(sorted_category_importance_scores.values())\n",
    "\n",
    "\n",
    "# Category importances plot\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(category_names)))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.grid(alpha=0.7, linestyle='dotted', zorder=0)\n",
    "bars = plt.barh(category_names, category_scores, color=colors, zorder=3)\n",
    "plt.xlabel('Category Importance Score', fontsize=20, fontname='Times New Roman')\n",
    "plt.ylabel('Category', fontsize=20, fontname='Times New Roman')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xticks(fontsize=20, fontname='Times New Roman')\n",
    "plt.yticks(fontsize=20, fontname='Times New Roman')\n",
    "\n",
    "# Add score to each bar\n",
    "for bar, score in zip(bars, category_scores):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{score:.3f}', \n",
    "             ha='left', va='center', color='black', fontsize=20, fontname='Times New Roman')\n",
    "# Enlarging x-axis for readability\n",
    "plt.xlim(right=max(category_scores) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/category_importance.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
